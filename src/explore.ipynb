{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# NLP Project"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Importing libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import re\n",
                "import nltk\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.metrics import classification_report, accuracy_score\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "import joblib\n",
                "import os"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load the dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> In this step, I’m loading the dataset containing labeled URLs to determine whether they are spam or not. The dataset is publicly available via a GitHub link. I’m using `pandas` to read the CSV directly from the URL.\n",
                "\n",
                "- This will help me inspect the data and prepare it for the next preprocessing step.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>url</th>\n",
                            "      <th>is_spam</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>https://briefingday.us8.list-manage.com/unsubs...</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>https://www.hvper.com/</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>https://briefingday.com/m/v4n3i4f3</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>https://briefingday.com/n/20200618/m#commentform</td>\n",
                            "      <td>False</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>https://briefingday.com/fan</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                                 url  is_spam\n",
                            "0  https://briefingday.us8.list-manage.com/unsubs...     True\n",
                            "1                             https://www.hvper.com/     True\n",
                            "2                 https://briefingday.com/m/v4n3i4f3     True\n",
                            "3   https://briefingday.com/n/20200618/m#commentform    False\n",
                            "4                        https://briefingday.com/fan     True"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Load dataset from the provided GitHub URL\n",
                "url = \"https://raw.githubusercontent.com/4GeeksAcademy/NLP-project-tutorial/main/url_spam.csv\"\n",
                "df = pd.read_csv(url)\n",
                "\n",
                "# Display the first few rows\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> Everything loaded correctly, moving to the next step. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preprocess the links"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> In this step, I’m preparing the URLs for training a machine learning model. Since the URLs are just strings, I need to convert them into a format that the model can understand.\n",
                "\n",
                "> Here's what I’ll do:\n",
                "- **Tokenize** the URLs by breaking them into meaningful parts using common punctuation marks.\n",
                "- **Remove stopwords** (even though they’re rare in URLs, this is good hygiene).\n",
                "- **Lemmatize** the tokens to reduce them to their base forms.\n",
                "- **Vectorize** the result using `TfidfVectorizer` so we can train an SVM later.\n",
                "\n",
                "> Finally, I’ll split the dataset into training and testing sets.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
                        "[nltk_data]   Package stopwords is already up-to-date!\n",
                        "[nltk_data] Downloading package wordnet to /home/vscode/nltk_data...\n",
                        "[nltk_data]   Package wordnet is already up-to-date!\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "((2399, 6335), (600, 6335))"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Download required NLTK data\n",
                "nltk.download('stopwords')\n",
                "nltk.download('wordnet')\n",
                "\n",
                "# Initialize tools\n",
                "stop_words = set(stopwords.words('english'))\n",
                "lemmatizer = WordNetLemmatizer()\n",
                "\n",
                "# Custom preprocessing function for URLs\n",
                "def preprocess_url(url):\n",
                "    # Split on punctuation and non-alphanumeric characters\n",
                "    tokens = re.split(r'\\W+', url.lower())\n",
                "    # Remove stopwords and lemmatize\n",
                "    clean_tokens = [lemmatizer.lemmatize(token) for token in tokens if token and token not in stop_words]\n",
                "    return ' '.join(clean_tokens)\n",
                "\n",
                "# Apply preprocessing to the URL column\n",
                "df['clean_url'] = df['url'].apply(preprocess_url)\n",
                "\n",
                "# Vectorize the cleaned URLs\n",
                "vectorizer = TfidfVectorizer()\n",
                "X = vectorizer.fit_transform(df['clean_url'])\n",
                "\n",
                "# Target variable\n",
                "y = df['is_spam']  # 1 = spam, 0 = not spam\n",
                "\n",
                "# Split into training and testing sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Confirm dimensions\n",
                "X_train.shape, X_test.shape\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> In this step, I prepared the dataset for machine learning by cleaning and transforming the URLs into numerical vectors. Here’s a breakdown of what I did:\n",
                "\n",
                "- **Tokenization:** I split each URL into smaller parts using punctuation marks like `.`, `/`, `-`, and `_`.\n",
                "- **Stopword Removal:** I removed common English stopwords that don’t help the model (e.g., “the”, “and”).\n",
                "- **Lemmatization:** I reduced each word to its root form to normalize the vocabulary (e.g., “running” → “run”).\n",
                "- **TF-IDF Vectorization:** I converted the processed URLs into a matrix of numerical features based on how important each token is across the dataset.\n",
                "\n",
                "> Finally, I split the dataset into:\n",
                "- **Training set:** 2,399 URLs\n",
                "- **Test set:** 600 URLs\n",
                "- **Feature space:** 6,335 unique tokens\n",
                "\n",
                "> This step ensures that my SVM model will receive clean, consistent, and meaningful inputs.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train a Support Vector Machine (SVM)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> In this step, I trained a Support Vector Machine (SVM) classifier using the default parameters.\n",
                "\n",
                "- The goal here is to establish a strong baseline model that can classify whether a given URL is spam or not based on its tokenized and vectorized form.\n",
                "\n",
                "> SVMs are effective for binary classification problems like this one and tend to perform well with high-dimensional data, which suits the TF-IDF representation we built in the previous step.\n",
                "\n",
                "- Let's check the model's accuracy and classification report to evaluate its performance on the test set.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Accuracy: 0.9583\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "       False       0.95      1.00      0.97       455\n",
                        "        True       0.99      0.83      0.91       145\n",
                        "\n",
                        "    accuracy                           0.96       600\n",
                        "   macro avg       0.97      0.92      0.94       600\n",
                        "weighted avg       0.96      0.96      0.96       600\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Initialize the SVM with default parameters\n",
                "svm_model = SVC()\n",
                "\n",
                "# Train the model\n",
                "svm_model.fit(X_train, y_train)\n",
                "\n",
                "# Make predictions on the test set\n",
                "y_pred = svm_model.predict(X_test)\n",
                "\n",
                "# Evaluate performance\n",
                "accuracy = accuracy_score(y_test, y_pred)\n",
                "print(f\"Accuracy: {accuracy:.4f}\")\n",
                "\n",
                "# Detailed classification report\n",
                "print(classification_report(y_test, y_pred))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> In this step, I trained a Support Vector Machine (SVM) using default hyperparameters on the preprocessed URL data. The model was then evaluated on the test set, and the performance metrics are very promising.\n",
                "\n",
                "- **Accuracy**: `95.83%` — This means the model correctly predicted whether a URL was spam or not in nearly 96% of the cases.\n",
                "- **Precision**:\n",
                "  - **False (not spam)**: `95%`\n",
                "  - **True (spam)**: `99%` — This shows that the model is excellent at detecting spam when it makes a positive prediction.\n",
                "- **Recall**:\n",
                "  - **False**: `100%` — The model perfectly recognized all non-spam links.\n",
                "  - **True**: `83%` — It missed some spam URLs, which I’ll try to improve in the next step through optimization.\n",
                "- **F1-Score**:\n",
                "  - **False**: `0.97`\n",
                "  - **True**: `0.91` — A solid balance between precision and recall for both classes.\n",
                "\n",
                "> Overall, the SVM model performs very well out of the box, but there's room to improve its ability to catch all spam cases. In Step 4, I’ll optimize the model’s hyperparameters to boost its performance further.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Hyperparameter Optimization"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> In this step, I optimized the Support Vector Machine by performing a grid search to find the best combination of hyperparameters. The goal is to improve the model's ability to detect spam links, especially those that were misclassified in the initial version.\n",
                "\n",
                "- I focused on tuning the `C` (regularization strength), `kernel` type, and `gamma` (kernel coefficient). These parameters directly impact the decision boundary and model flexibility. \n",
                "- After finding the best combination, I retrained the model and evaluated its performance.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
                        "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   0.2s\n",
                        "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   0.2s\n",
                        "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   0.2s\n",
                        "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   0.2s\n",
                        "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   0.2s\n",
                        "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.3s\n",
                        "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.3s\n",
                        "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.3s\n",
                        "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.3s\n",
                        "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.3s\n",
                        "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=   0.4s\n",
                        "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=   0.4s\n",
                        "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=   0.4s\n",
                        "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=   0.4s\n",
                        "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=   0.4s\n",
                        "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   0.2s\n",
                        "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   0.2s\n",
                        "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   0.2s\n",
                        "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   0.2s\n",
                        "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   0.2s\n",
                        "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.2s\n",
                        "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.2s\n",
                        "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.2s\n",
                        "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.2s\n",
                        "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.2s\n",
                        "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=   0.2s\n",
                        "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=   0.2s\n",
                        "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=   0.2s\n",
                        "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=   0.2s\n",
                        "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=   0.2s\n",
                        "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   0.1s\n",
                        "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   0.1s\n",
                        "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   0.2s\n",
                        "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   0.2s\n",
                        "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   0.2s\n",
                        "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.3s\n",
                        "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.3s\n",
                        "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.3s\n",
                        "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.3s\n",
                        "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.3s\n",
                        "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=   0.4s\n",
                        "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=   0.4s\n",
                        "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=   0.4s\n",
                        "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=   0.4s\n",
                        "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=   0.4s\n",
                        "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   0.2s\n",
                        "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   0.1s\n",
                        "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   0.1s\n",
                        "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   0.2s\n",
                        "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   0.2s\n",
                        "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.2s\n",
                        "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.2s\n",
                        "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.2s\n",
                        "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.2s\n",
                        "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.2s\n",
                        "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=   0.2s\n",
                        "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=   0.2s\n",
                        "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=   0.2s\n",
                        "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=   0.2s\n",
                        "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=   0.2s\n",
                        "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   0.1s\n",
                        "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   0.1s\n",
                        "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   0.1s\n",
                        "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   0.1s\n",
                        "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   0.2s\n",
                        "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.3s\n",
                        "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.3s\n",
                        "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.3s\n",
                        "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.3s\n",
                        "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.3s\n",
                        "[CV] END .....................C=10, gamma=scale, kernel=poly; total time=   0.4s\n",
                        "[CV] END .....................C=10, gamma=scale, kernel=poly; total time=   0.4s\n",
                        "[CV] END .....................C=10, gamma=scale, kernel=poly; total time=   0.4s\n",
                        "[CV] END .....................C=10, gamma=scale, kernel=poly; total time=   0.4s\n",
                        "[CV] END .....................C=10, gamma=scale, kernel=poly; total time=   0.4s\n",
                        "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   0.1s\n",
                        "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   0.1s\n",
                        "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   0.1s\n",
                        "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   0.2s\n",
                        "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   0.1s\n",
                        "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.2s\n",
                        "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.2s\n",
                        "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.2s\n",
                        "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.2s\n",
                        "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.2s\n",
                        "[CV] END ......................C=10, gamma=auto, kernel=poly; total time=   0.2s\n",
                        "[CV] END ......................C=10, gamma=auto, kernel=poly; total time=   0.2s\n",
                        "[CV] END ......................C=10, gamma=auto, kernel=poly; total time=   0.2s\n",
                        "[CV] END ......................C=10, gamma=auto, kernel=poly; total time=   0.2s\n",
                        "[CV] END ......................C=10, gamma=auto, kernel=poly; total time=   0.2s\n",
                        "Best Parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
                        "Accuracy: 0.9683333333333334\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "       False       0.97      0.99      0.98       455\n",
                        "        True       0.96      0.90      0.93       145\n",
                        "\n",
                        "    accuracy                           0.97       600\n",
                        "   macro avg       0.97      0.95      0.96       600\n",
                        "weighted avg       0.97      0.97      0.97       600\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Define the parameter grid\n",
                "param_grid = {\n",
                "    'C': [0.1, 1, 10],\n",
                "    'kernel': ['linear', 'rbf', 'poly'],\n",
                "    'gamma': ['scale', 'auto']\n",
                "}\n",
                "\n",
                "# Initialize SVM\n",
                "svc = SVC()\n",
                "\n",
                "# Run Grid Search\n",
                "grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy', verbose=2)\n",
                "grid_search.fit(X_train, y_train)\n",
                "\n",
                "# Get best model\n",
                "best_svm = grid_search.best_estimator_\n",
                "\n",
                "# Evaluate best model\n",
                "y_pred_optimized = best_svm.predict(X_test)\n",
                "\n",
                "# Print results\n",
                "print(\"Best Parameters:\", grid_search.best_params_)\n",
                "print(\"Accuracy:\", accuracy_score(y_test, y_pred_optimized))\n",
                "print(classification_report(y_test, y_pred_optimized))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> In this step, I used GridSearchCV to find the best combination of hyperparameters for the SVM model. I tested different values of `C`, `kernel`, and `gamma` using 5-fold cross-validation. The best parameters found were:\n",
                "\n",
                "- **C**: 10  \n",
                "- **Kernel**: 'rbf'  \n",
                "- **Gamma**: 'scale'\n",
                "\n",
                "> After training the model with these optimized parameters, I achieved the following results:\n",
                "\n",
                "- **Accuracy**: 0.9683\n",
                "- **Precision (spam)**: 0.96\n",
                "- **Recall (spam)**: 0.90\n",
                "- **F1-Score (spam)**: 0.93\n",
                "\n",
                "> This shows a clear improvement over the baseline SVM model from Step 3, especially in terms of recall for the spam class, which means the optimized model is better at identifying actual spam links. The overall performance is okay. \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> I'll move to the final step"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save the Optimized SVM Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['models/url_vectorizer.pkl']"
                        ]
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Ensure the models folder exists\n",
                "os.makedirs(\"models\", exist_ok=True)\n",
                "\n",
                "# Save the best model from GridSearchCV\n",
                "joblib.dump(grid_search.best_estimator_, \"models/best_svm_model.pkl\")\n",
                "\n",
                "# Also save the vectorizer so it can be reused for prediction\n",
                "joblib.dump(vectorizer, \"models/url_vectorizer.pkl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> To finalize the project, I saved the optimized SVM model using the `joblib` library. This will allow me to reuse the trained model later for inference without needing to retrain it. I also saved the fitted `TfidfVectorizer`, which is essential to transform new input data in the same way as during training.\n",
                "\n",
                "> Files saved:\n",
                "- `models/best_svm_model.pkl`: the optimized SVM classifier.\n",
                "- `models/url_vectorizer.pkl`: the fitted Tfidf vectorizer for preprocessing URLs.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> Aaaaaand its finished."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
